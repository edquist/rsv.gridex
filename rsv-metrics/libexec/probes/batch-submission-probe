#!/usr/bin/python

import os
import re
import sys
import time
import shutil
import datetime
import tempfile
import subprocess

import rsvprobe

def run_with_subprocess(cmd):
    """Run a command using subprocess, returning a tuple of (output, error,
    returncode) where output and error are the contents of stdout and stderr,
    respectively. Forces 'C' locale in case we need to parse the output.

    """
    new_env = dict(os.environ, LC_ALL='C')
    try:
        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE, env=new_env)
        output, error = proc.communicate()
        returncode = proc.returncode
    except OSError, (errno, strerror):
        output, error = "", "Could not execute %s: %s" % (cmd[0], strerror)
        returncode = 1

    return (output, error, returncode)

def run_in_tempdir(fn):
    olddir = os.getcwd()
    tmpd = tempfile.mkdtemp()
    os.chdir(tmpd)
    try:
        ret = fn()
    finally:
        os.chdir(olddir)
        shutil.rmtree(tmpd)
    return ret


class BatchSubmissionProbe(rsvprobe.RSVProbe):

    def __init__(self):
        rsvprobe.RSVProbe.__init__(self)
        self.metric = ""
        metric = rsvprobe.RSVMetric("OSG-HTCondor-CE",
                                    "org.osg.batch.test-submission",
                                    rsvprobe.RSVMetric.STATUS)
        metric.service_version = ">= OSG HTCondor-CE 1.0.0"
        metric.probe_type = "OSG-HTCondor-CE"
        self.supported_metrics = [metric]
        self.details = []

#   def parseopt(self):
#       options, optlist, remainder = rsvprobe.RSVProbe.parseopt(self)

    def get_probe_timeout(self):
        # sadly there does not seem to be a better way to get this...
        # eventually, it would be nice to have RSV put this into the
        # probe's environment.
        import rsv.RSV
        import optparse

        # RSV constructor expects at least options.verbose to be set
        options = optparse.OptionParser()
        options.verbose = False

        rsv = rsv.RSV.RSV(options)
        metrics = rsv.get_metric_info()
        if self.metric:
            metric_timeout = metrics[self.metric].get_timeout()
            if metric_timeout is not None:
                return int(metric_timeout)

        if rsv.config.has_option('rsv', 'job-timeout'):
            rsv_timeout = rsv.config.get('rsv', 'job-timeout')
            if rsv_timeout is not None:
                return int(rsv_timeout)

    def setup_timeouts(self):
        default_probe_timeout   = 1200
        poll_interval           = 5
        cancelled_timeout_ratio = 2.0   # multiple of poll_interval
        completed_timeout_ratio = 0.90  # multiple of cancelled_timeout
        routed_timeout_ratio    = 0.75  # multiple of cancelled_timeout

        # time when whe expect rsv to kill the probe
        probe_timeout = self.get_probe_timeout() or default_probe_timeout

        # time to give up waiting for job to cancel, before rsv kills the probe
        cancelled_timeout = int(
            probe_timeout - poll_interval * cancelled_timeout_ratio
        )

        # time to cancel job, before job gets routed
        routed_timeout = int(cancelled_timeout * routed_timeout_ratio)

        # time to cancel job, before executing job completes
        completed_timeout = int(cancelled_timeout * completed_timeout_ratio)

        self.probe_timeout     = probe_timeout
        self.poll_interval     = poll_interval
        self.cancelled_timeout = cancelled_timeout
        self.completed_timeout = completed_timeout
        self.routed_timeout    = routed_timeout

    def routed_timeout_expired(self):
        return time.time() > self.starttime + self.routed_timeout

    def completed_timeout_expired(self):
        return time.time() > self.starttime + self.completed_timeout

    def cancelled_timeout_expired(self):
        return time.time() > self.starttime + self.cancelled_timeout

    def details_str(self):
        return ''.join('\n\n' + x for x in self.details)

    def add_details(self, msg, add_timestamp=True):
        if add_timestamp:
            now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            self.details.append("%s: %s" % (now, msg))
        else:
            self.details.append(msg)

    def success(self, msg):
        self.return_ok(msg + self.details_str())

    def warning(self, msg):
        self.return_warning(msg + self.details_str())

    def run(self):
        """Main routine for the probe"""
        self.parseopt()
        self.setup_timeouts()
        self.starttime = time.time()
        self.timeouts_hit = set()

        if not self.do_condor_submit():
            self.warning("Condor submit failed.")

        # wait for job to be routed
        while not (self.job_has_been_routed() or
                   self.job_held_or_started_or_terminated() or
                   self.routed_timeout_expired()):
            time.sleep(self.poll_interval)

        if (self.job_has_been_routed(1)):
            # wait for job to run and finish
            while not (self.job_held_or_terminated() or
                       self.completed_timeout_expired()):
                time.sleep(self.poll_interval)

            if not self.job_held_or_terminated():
                self.cancel_job('completed')
        else:
            self.cancel_job('routed')

        if self.timeouts_hit:
            # cancel was attempted, wait for clean removal
            while self.job_in_condor_q() and not (
                    self.job_held_or_terminated() or
                    self.cancelled_timeout_expired()):
                time.sleep(self.poll_interval)
            if self.job_in_condor_q():
                self.timeouts_hit.add('cancelled')
                self.add_details("Timed out waiting for job to cancel.")
            else:
                self.add_details("Job successfully removed.")

        self.add_details("--- Job Log ---\n\n" + self.read_joblog(), 0)

        self.do_status_report()

    def do_status_report(self):
        stats = self.parse_joblog()

        if 'terminated' in stats:
            self.success("Test job successfully completed.")
        elif 'held' in stats:
            self.warning("Test job was put on Hold status.")
        elif 'shadow_ex' in stats:
            self.warning("Test job had a shadow exception.")
        elif 'executing' in stats:
            self.success("Test job successfully started on batch system.")
        elif self.job_has_been_routed(1):
            self.success("Test job was successfully routed.")
        elif 'aborted' in stats:
            self.warning("Test job was aborted.")
        else:
            self.warning("Test job not routed within timeout window.")

    def submit_file_txt(self):
        sleep_time = 1
        host = self.host
        metric = self.metric

        txt = """
        universe = grid
        grid_resource = condor %s %s:9619

        executable = /bin/sleep
        arguments = %d
        output = /dev/null
        error = /dev/null
        log = %s.log

        ShouldTransferFiles = YES
        WhenToTransferOutput = ON_EXIT

        use_x509userproxy = true
        +Owner=undefined
        queue
        """ % (host, host, sleep_time, metric)

        return re.sub(r'\n +', r'\n', txt)

    def cmd_details(self, msg, out, err):
        self.add_details(
            "%s.  Command output:\n"
            "---\n%s\n---\n%s\n---\n" % (msg, out.strip(), err.strip())
        )

    def do_condor_submit(self):
        submit_filename = "%s.sub" % self.metric
        sf = open(submit_filename, "w")
        sf.write(self.submit_file_txt())
        sf.close()

        out,err,ret = run_with_subprocess(["condor_submit", submit_filename])

        if ret != 0:
            self.cmd_details("Problem submitting job to condor", out, err)
            return False

        # Determine the job cluster ID
        match = re.search("submitted to cluster (\d+)\.", out)
        if match:
            self.local_job_id = match.group(1)
            self.add_details("Condor job cluster ID: " + self.local_job_id)
            return True
        else:
            self.add_details("Could not determine job cluster ID "
                                "from output:\n" + out)
            return False

    def cancel_job(self, timeout_hit):
        self.timeouts_hit.add(timeout_hit)
        self.add_details("Job %s not %s within timeout, cancelling." % (
                            self.local_job_id, timeout_hit))
        out,err,ret = run_with_subprocess(["condor_rm", self.local_job_id])

        if ret != 0:
            self.cmd_details("Problem removing condor job", out, err)

    def job_in_condor_q(self):
        out,err,ret = run_with_subprocess([
            "condor_q", "-format", "%s", "ClusterId", self.local_job_id
        ])

        if out == self.local_job_id:
            return True
        #elif out or ret:
        #    self.cmd_details("Unexpected condor_q results", out, err)

    def get_grid_job_id(self):
        if hasattr(self, 'grid_job_id'):
            return self.grid_job_id

        out,err,ret = run_with_subprocess([
            "condor_q", "-format", "%s", "GridJobId", self.local_job_id
        ])

        # can also extract condor vs globus grid type here
        match = re.search(r' (\d+\.\d+)$', out)
        if match:
            self.grid_job_id = match.group(1)
            self.add_details("Condor GridJobId: %s" % self.grid_job_id)
            return self.grid_job_id
        else:
            return False

    def job_has_been_routed(self, no_retry=False):
        """ for now just poll to see if routed attr appears """

        if hasattr(self, 'routed_job_id'):
            return self.routed_job_id
        elif no_retry:
            return False

        if not self.get_grid_job_id():
            return False

        ce = self.host
        out,err,ret = run_with_subprocess([
            "condor_q", "-name", ce, "-pool", ce + ":9619",
            "-format", "%s", "RoutedToJobId", self.grid_job_id
        ])

        match = re.search(r'^(\d+\.\d+)$', out)
        if match:
            self.routed_job_id = match.group(1)
            self.add_details("Grid RoutedToJobId: %s" % self.routed_job_id)
            return self.routed_job_id
        else:
            return False
        
    def read_joblog(self):
        return open("%s.log" % self.metric).read()

    def parse_joblog(self):
        log_txt = self.read_joblog()
        status_map = {
            'executing'  :  1,
            'exe_error'  :  2,
            'terminated' :  5,
            'shadow_ex'  :  7,
            'aborted'    :  9,
            'held'       : 12,
            'grid_sub'   : 27,
        }
        stats = set()
        for name,id in status_map.items():
            if re.search(r'^%03d ' % id, log_txt, re.M):
                stats.add(name)

        return stats

    def job_held_or_started_or_terminated(self):
        """
        phase 2: watch logs for:
            (1) "Job executing on host:"
            (9) "Job was aborted by the user."
            (5) "Job terminated."
            (2,21,?) "error" ?

        or:
            Submit                  0
            Execute                 1
            Executable error        2
            Checkpointed            3
            Job evicted             4
            Job terminated          5
            Image size              6
            Shadow exception        7
            Generic                 8
            Job aborted             9
            Job suspended           10
            Job unsuspended         11
            Job held                12
            Job released            13
            Node execute            14
            Node terminated         15
            Post script terminated  16
            Globus submit           17
            Globus submit failed    18
            Globus resource up      19
            Globus resource down    20
            Remote error            21
        """

        stats = self.parse_joblog()
        return stats & set(['held', 'executing', 'terminated', 'aborted'])

    def job_held_or_terminated(self):
        stats = self.parse_joblog()
        return stats & set(['held', 'terminated', 'aborted'])

def main():
    probe = BatchSubmissionProbe()
    probe.run()
    return 0

if __name__ == '__main__':
    sys.exit(run_in_tempdir(main))

